GB, 梯度提升，通过进行M次迭代，每次迭代产生一个回归树模型，我们需要让每次迭代生成的模型对训练集的损失函数最小，而如何让损失函数越来越小呢？我们采用梯度下降的方法，在每次迭代时通过向损失函数的负梯度方向移动来使得损失函数越来越小，这样我们就可以得到越来越精确的模型。 







假设GBDT模型T有4棵回归树构成：t1，t2，t3，t4，样本标签为Y（y1,y2,y3,.....yn）



设定该模型的误差函数为L，并且为SquaredError，则整体样本的误差推导如下：



