# 决策树的数学原理

通俗来讲，决策树的构建过程就是将数据根据其特征分布划分到不同的区域，使得同一个区域的样本有尽可能一致的类别标签。在决策树构建的过程中，我们需要一个衡量标准来确定每次数据划分所带来的收益，这个标准就是信息熵，以0-1二分类问题为例，衡量一个节点的信息熵公式如下：

![](/assets/entropy.png)

其中p为当前节点中正样本的比例，Entropy越大，说明节点的样本越杂，因此Entropy越小越好。假设我们每次对数据划分都是将数据一分为二，分别为left和right， 分裂的收益就是分裂前节点的Entropy减去这两个节点的Entropy的加权和。即：Entropy\\(parent\\) - Prob\\(left\\) \*Entropy\\(left\\) -Prob\\(right\\) \*Entropy\\(right\\)，这个值越大越好。这个收益，学术上我们称作“信息增益”。其中Prob\\(left\\)为左节点的样比例，Prob\\(right\\)为右节点的样本比例。 由于单纯使用信息增益作为标准来构建决策树，容易导致过拟合的问题。因此前辈们又引入了“信息增益率”，以及对树进行剪枝等方式来优化树的创建过程。

决策树的构建过程：

1，遍历所有特征，计算每个特征的信息增益

2，选取信息增益最大的特征，将特征分组

3，对于每个分组，递归进行上面步骤1和步骤2

4，直到用完所有特征或者分组中已经样本都属于同一个分类

关于模型我们讨论的决策树，是通过将数据划分到不同的互不重合区域\\(对应于树的生长过程\\)，使得每个区域的样本有尽可能一直的类标签\\(对应于目标函数的最大化\\)的模型。只是一种建模方法，而与要优化的目标没有必然关系。

关于目标函数

而上文中我们为了解释信息熵的原理而引入的二项分布，以及基于此构造的损失函数Loss，则是我们的目标函数。我们的目标是最大化该函数的值。

关于模型参数

有了目标函数，才有模型的参数。模型参数是在模型结构的约束下，依据给定的训练数据，使得目标函数取的最大值时的函数参数。

三者关系

一个模型可以有很多个目标，一个目标可以通过不同的模型去优化。即模型结构与目标函数是独立的，互补依赖，互补干涉。

而参数则共同依赖于模型结构和目标函数，有什么样的目标函数，就有什么样的参数形式；而模型的结构则决定了最终的参数结果。

模型极易出现过拟合，没什么用处。因此工业上在使用决策树模型来建模数据时，往往要加上一些限制和约束。比如说限制树的深度、限制树叶子节点的个数、为模型参数加上L2或者L1的显式正则，更或者我们使用多棵而不是仅仅使用单棵树来建模数据。



        使用多棵树来建模时，树的组合方式又有不同的选择，比如：boosting，bagging。使用Boosting来组合决策树的模型我们称之为GBDT\(gradient boosting decision tree\)；而使用bagging方式来组合决策树的模型我们则称之为random Forest\(随机森林\)。这也是工业界最常使用的两种树模型,我们统称为TreebasedModel......





